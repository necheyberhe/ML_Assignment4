{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c44d416d",
   "metadata": {},
   "source": [
    "Step 1: Setup and Data Download\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f00823dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Notebook-cell version: VGG19 transfer learning on Oxford 102 Flowers\n",
    "- Random split 50/25/25\n",
    "- Run twice by calling run_experiment(split_seed=1) and run_experiment(split_seed=2)\n",
    "- Saves curves + checkpoint into out_dir\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Dataset, Subset\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "268fc0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import scipy.io as sio\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class Flowers102Local(Dataset):\n",
    "    \"\"\"\n",
    "    Local Oxford 102 Flowers dataset:\n",
    "    root/\n",
    "      jpg/\n",
    "      imagelabels.mat\n",
    "      setid.mat\n",
    "    \"\"\"\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = Path(root)\n",
    "        self.img_dir = self.root / \"jpg\"\n",
    "        self.transform = transform\n",
    "\n",
    "        mat = sio.loadmat(self.root / \"imagelabels.mat\")\n",
    "        labels = mat[\"labels\"].squeeze().astype(int)  # 1..102\n",
    "        self.labels = (labels - 1).tolist()           # 0..101\n",
    "\n",
    "        self.image_paths = [\n",
    "            self.img_dir / f\"image_{i:05d}.jpg\"\n",
    "            for i in range(1, len(self.labels) + 1)\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert(\"RGB\")\n",
    "        y = self.labels[idx]\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f6b8873e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Dataset utilities\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class SplitIndices:\n",
    "    train: List[int]\n",
    "    val: List[int]\n",
    "    test: List[int]\n",
    "\n",
    "\n",
    "class TransformOverrideDataset(Dataset):\n",
    "    def __init__(self, base: Dataset, transform):\n",
    "        self.base = base\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.base[idx]\n",
    "        if self.transform is not None:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e65cddeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_flowers102_all(root: str) -> Dataset:\n",
    "    return Flowers102Local(root=root, transform=None)\n",
    "\n",
    "\n",
    "def make_random_split_indices(n: int, split_seed: int) -> SplitIndices:\n",
    "    g = torch.Generator().manual_seed(split_seed)\n",
    "    perm = torch.randperm(n, generator=g).tolist()\n",
    "\n",
    "    n_train = int(0.50 * n)\n",
    "    n_val = int(0.25 * n)\n",
    "    n_test = n - n_train - n_val\n",
    "\n",
    "    train_idx = perm[:n_train]\n",
    "    val_idx = perm[n_train:n_train + n_val]\n",
    "    test_idx = perm[n_train + n_val:n_train + n_val + n_test]\n",
    "\n",
    "    return SplitIndices(train=train_idx, val=val_idx, test=test_idx)\n",
    "\n",
    "\n",
    "def save_split_indices(path: str, split: SplitIndices) -> None:\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"train\": split.train, \"val\": split.val, \"test\": split.test}, f)\n",
    "\n",
    "\n",
    "def load_split_indices(path: str) -> SplitIndices:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        obj = json.load(f)\n",
    "    return SplitIndices(train=obj[\"train\"], val=obj[\"val\"], test=obj[\"test\"])\n",
    "\n",
    "\n",
    "def build_transforms(img_size: int = 224):\n",
    "    imagenet_mean = [0.485, 0.456, 0.406]\n",
    "    imagenet_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "    train_tf = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.02),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "    ])\n",
    "\n",
    "    eval_tf = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=imagenet_mean, std=imagenet_std),\n",
    "    ])\n",
    "\n",
    "    return train_tf, eval_tf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "681b4fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_loaders(\n",
    "    root: str,\n",
    "    batch_size: int,\n",
    "    num_workers: int,\n",
    "    split_seed: int,\n",
    "    split_cache_path: str,\n",
    "    img_size: int = 224,\n",
    ") -> Tuple[DataLoader, DataLoader, DataLoader, SplitIndices]:\n",
    "    base_all = load_flowers102_all(root=root)\n",
    "    n = len(base_all)\n",
    "\n",
    "    if os.path.isfile(split_cache_path):\n",
    "        split = load_split_indices(split_cache_path)\n",
    "    else:\n",
    "        split = make_random_split_indices(n=n, split_seed=split_seed)\n",
    "        save_split_indices(split_cache_path, split)\n",
    "\n",
    "    train_tf, eval_tf = build_transforms(img_size=img_size)\n",
    "\n",
    "    train_ds = TransformOverrideDataset(Subset(base_all, split.train), transform=train_tf)\n",
    "    val_ds   = TransformOverrideDataset(Subset(base_all, split.val), transform=eval_tf)\n",
    "    test_ds  = TransformOverrideDataset(Subset(base_all, split.test), transform=eval_tf)\n",
    "\n",
    "    # In Windows notebooks, num_workers>0 can cause issues. Use 0 unless you know it's stable.\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,\n",
    "                              num_workers=num_workers, pin_memory=True)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=batch_size, shuffle=False,\n",
    "                              num_workers=num_workers, pin_memory=True)\n",
    "    test_loader  = DataLoader(test_ds, batch_size=batch_size, shuffle=False,\n",
    "                              num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76ae9df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Model\n",
    "# -----------------------------\n",
    "def build_vgg19_classifier(num_classes: int = 102, freeze_features: bool = True) -> nn.Module:\n",
    "    model = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1)\n",
    "\n",
    "    if freeze_features:\n",
    "        for p in model.features.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "    in_features = model.classifier[-1].in_features\n",
    "    model.classifier[-1] = nn.Linear(in_features, num_classes)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a9089ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Train/Eval loops\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, loader: DataLoader, device: torch.device, criterion: nn.Module) -> Tuple[float, float]:\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += x.size(0)\n",
    "\n",
    "    return total_loss / max(1, total), correct / max(1, total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67efd1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(\n",
    "    model: nn.Module,\n",
    "    loader: DataLoader,\n",
    "    device: torch.device,\n",
    "    criterion: nn.Module,\n",
    "    optimizer: optim.Optimizer,\n",
    ") -> Tuple[float, float]:\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        y = y.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += x.size(0)\n",
    "\n",
    "    return total_loss / max(1, total), correct / max(1, total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "568d4d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_curves(out_dir: str, history: Dict[str, List[float]]) -> None:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(history[\"train_acc\"], label=\"train\")\n",
    "    plt.plot(history[\"val_acc\"], label=\"val\")\n",
    "    plt.plot(history[\"test_acc\"], label=\"test\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"accuracy_vgg19.png\"), dpi=160)\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(history[\"train_loss\"], label=\"train\")\n",
    "    plt.plot(history[\"val_loss\"], label=\"val\")\n",
    "    plt.plot(history[\"test_loss\"], label=\"test\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"cross_entropy\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(out_dir, \"loss_vgg19.png\"), dpi=160)\n",
    "    plt.close()\n",
    "\n",
    "    with open(os.path.join(out_dir, \"history_vgg19.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(history, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8154c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_checkpoint(path: str, model: nn.Module, optimizer: optim.Optimizer, epoch: int, best_val_acc: float) -> None:\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    torch.save(\n",
    "        {\"epoch\": epoch, \"model_state\": model.state_dict(),\n",
    "         \"optimizer_state\": optimizer.state_dict(), \"best_val_acc\": best_val_acc},\n",
    "        path,\n",
    "    )\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_proba(model: nn.Module, images: torch.Tensor, device: torch.device) -> torch.Tensor:\n",
    "    model.eval()\n",
    "    images = images.to(device)\n",
    "    logits = model(images)\n",
    "    return torch.softmax(logits, dim=1).cpu()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b6481fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# Notebook-friendly \"main\"\n",
    "# -----------------------------\n",
    "def run_experiment(\n",
    "    data_root: str = \"./data\",\n",
    "    out_dir: str = \"./results/vgg19_seed1\",\n",
    "    epochs: int = 35,\n",
    "    batch_size: int = 32,\n",
    "    lr: float = 1e-4,\n",
    "    weight_decay: float = 0.0,\n",
    "    num_workers: int = 0,         # NOTE: 0 is safest in Windows notebooks\n",
    "    img_size: int = 224,\n",
    "    split_seed: int = 1,\n",
    "    freeze_features: bool = True,\n",
    "    early_stop_patience: int = 7,\n",
    "    device: str = \"cuda\",\n",
    "):\n",
    "    if device == \"cuda\" and not torch.cuda.is_available():\n",
    "        print(\"CUDA requested but not available. Falling back to CPU.\")\n",
    "        device = \"cpu\"\n",
    "    device_t = torch.device(device)\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    split_cache_path = os.path.join(out_dir, f\"split_indices_seed_{split_seed}.json\")\n",
    "\n",
    "    set_seed(split_seed)\n",
    "\n",
    "    train_loader, val_loader, test_loader, _split = make_loaders(\n",
    "        root=data_root,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        split_seed=split_seed,\n",
    "        split_cache_path=split_cache_path,\n",
    "        img_size=img_size,\n",
    "    )\n",
    "\n",
    "    model = build_vgg19_classifier(num_classes=102, freeze_features=freeze_features).to(device_t)\n",
    "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(trainable_params, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    history = {k: [] for k in [\"train_loss\",\"val_loss\",\"test_loss\",\"train_acc\",\"val_acc\",\"test_acc\"]}\n",
    "\n",
    "    best_val_acc = -1.0\n",
    "    best_val_loss = math.inf\n",
    "    epochs_no_improve = 0\n",
    "    ckpt_path = os.path.join(out_dir, \"best_vgg19.pt\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        tr_loss, tr_acc = train_one_epoch(model, train_loader, device_t, criterion, optimizer)\n",
    "        va_loss, va_acc = evaluate(model, val_loader, device_t, criterion)\n",
    "        te_loss, te_acc = evaluate(model, test_loader, device_t, criterion)\n",
    "\n",
    "        history[\"train_loss\"].append(tr_loss)\n",
    "        history[\"val_loss\"].append(va_loss)\n",
    "        history[\"test_loss\"].append(te_loss)\n",
    "        history[\"train_acc\"].append(tr_acc)\n",
    "        history[\"val_acc\"].append(va_acc)\n",
    "        history[\"test_acc\"].append(te_acc)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:03d}/{epochs} | \"\n",
    "            f\"train loss {tr_loss:.4f} acc {tr_acc:.4f} | \"\n",
    "            f\"val loss {va_loss:.4f} acc {va_acc:.4f} | \"\n",
    "            f\"test loss {te_loss:.4f} acc {te_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        if va_acc > best_val_acc:\n",
    "            best_val_acc = va_acc\n",
    "            save_checkpoint(ckpt_path, model, optimizer, epoch, best_val_acc)\n",
    "\n",
    "        if va_loss < best_val_loss - 1e-6:\n",
    "            best_val_loss = va_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= early_stop_patience:\n",
    "                print(f\"Early stopping triggered (patience={early_stop_patience}).\")\n",
    "                break\n",
    "\n",
    "    save_curves(out_dir, history)\n",
    "\n",
    "    best_epoch = int(np.argmax(history[\"val_acc\"])) + 1\n",
    "    print(\"\\nSummary\")\n",
    "    print(f\"- Split seed: {split_seed}\")\n",
    "    print(f\"- Best val acc: {max(history['val_acc']):.4f} (epoch {best_epoch})\")\n",
    "    print(f\"- Last test acc: {history['test_acc'][-1]:.4f}\")\n",
    "    print(f\"- Saved: {os.path.join(out_dir, 'accuracy_vgg19.png')}\")\n",
    "    print(f\"- Saved: {os.path.join(out_dir, 'loss_vgg19.png')}\")\n",
    "    print(f\"- Checkpoint: {ckpt_path}\")\n",
    "    print(f\"- Split indices: {split_cache_path}\")\n",
    "\n",
    "    return history, ckpt_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bb88ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001/35 | train loss 3.1448 acc 0.2865 | val loss 1.5717 acc 0.6116 | test loss 1.6156 acc 0.6099\n",
      "Epoch 002/35 | train loss 1.3604 acc 0.6412 | val loss 0.9499 acc 0.7538 | test loss 0.9626 acc 0.7480\n",
      "Epoch 003/35 | train loss 0.8579 acc 0.7587 | val loss 0.7738 acc 0.7831 | test loss 0.7844 acc 0.7871\n",
      "Epoch 004/35 | train loss 0.6324 acc 0.8190 | val loss 0.6457 acc 0.8217 | test loss 0.6614 acc 0.8193\n",
      "Epoch 005/35 | train loss 0.4731 acc 0.8625 | val loss 0.6027 acc 0.8334 | test loss 0.6209 acc 0.8330\n",
      "Epoch 006/35 | train loss 0.3598 acc 0.8898 | val loss 0.5875 acc 0.8315 | test loss 0.6244 acc 0.8384\n",
      "Epoch 007/35 | train loss 0.3036 acc 0.9108 | val loss 0.5879 acc 0.8344 | test loss 0.6061 acc 0.8364\n",
      "Epoch 008/35 | train loss 0.2374 acc 0.9287 | val loss 0.5396 acc 0.8520 | test loss 0.5580 acc 0.8540\n",
      "Epoch 009/35 | train loss 0.2295 acc 0.9304 | val loss 0.5612 acc 0.8447 | test loss 0.5477 acc 0.8579\n",
      "Epoch 010/35 | train loss 0.1890 acc 0.9436 | val loss 0.5072 acc 0.8500 | test loss 0.5317 acc 0.8521\n",
      "Epoch 011/35 | train loss 0.1515 acc 0.9555 | val loss 0.5295 acc 0.8549 | test loss 0.5339 acc 0.8657\n",
      "Epoch 012/35 | train loss 0.1302 acc 0.9617 | val loss 0.5341 acc 0.8539 | test loss 0.5575 acc 0.8599\n",
      "Epoch 013/35 | train loss 0.1241 acc 0.9619 | val loss 0.5429 acc 0.8544 | test loss 0.5619 acc 0.8540\n",
      "Epoch 014/35 | train loss 0.1116 acc 0.9651 | val loss 0.5485 acc 0.8534 | test loss 0.5832 acc 0.8491\n",
      "Epoch 015/35 | train loss 0.0910 acc 0.9736 | val loss 0.5320 acc 0.8622 | test loss 0.5332 acc 0.8608\n",
      "Epoch 016/35 | train loss 0.0937 acc 0.9722 | val loss 0.5345 acc 0.8578 | test loss 0.5502 acc 0.8662\n",
      "Epoch 017/35 | train loss 0.0690 acc 0.9805 | val loss 0.5832 acc 0.8608 | test loss 0.6090 acc 0.8491\n",
      "Early stopping triggered (patience=7).\n",
      "\n",
      "Summary\n",
      "- Split seed: 1\n",
      "- Best val acc: 0.8622 (epoch 15)\n",
      "- Last test acc: 0.8491\n",
      "- Saved: ./results/vgg19_seed1\\accuracy_vgg19.png\n",
      "- Saved: ./results/vgg19_seed1\\loss_vgg19.png\n",
      "- Checkpoint: ./results/vgg19_seed1\\best_vgg19.pt\n",
      "- Split indices: ./results/vgg19_seed1\\split_indices_seed_1.json\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Reader needs file name or open file-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\miniconda3\\Lib\\site-packages\\scipy\\io\\matlab\\_mio.py:39\u001b[39m, in \u001b[36m_open_file\u001b[39m\u001b[34m(file_like, appendmat, mode)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# Probably \"not found\"\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data\\\\imagelabels.mat'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m h1, ckpt1 = run_experih1, ckpt1 = run_experiment(\n\u001b[32m      2\u001b[39m     data_root=\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mC:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mUsers\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mhp\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mDownloads\u001b[39m\u001b[33m\\\u001b[39m\u001b[33m102flowers\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      3\u001b[39m     split_seed=\u001b[32m1\u001b[39m,\n\u001b[32m      4\u001b[39m     out_dir=\u001b[33m\"\u001b[39m\u001b[33m./results/vgg19_seed1\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m h2, ckpt2 = \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_seed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./results/vgg19_seed2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mrun_experiment\u001b[39m\u001b[34m(data_root, out_dir, epochs, batch_size, lr, weight_decay, num_workers, img_size, split_seed, freeze_features, early_stop_patience, device)\u001b[39m\n\u001b[32m     24\u001b[39m split_cache_path = os.path.join(out_dir, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33msplit_indices_seed_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit_seed\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.json\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     26\u001b[39m set_seed(split_seed)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m train_loader, val_loader, test_loader, _split = \u001b[43mmake_loaders\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_root\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit_seed\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43msplit_cache_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43msplit_cache_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimg_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m model = build_vgg19_classifier(num_classes=\u001b[32m102\u001b[39m, freeze_features=freeze_features).to(device_t)\n\u001b[32m     38\u001b[39m trainable_params = [p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model.parameters() \u001b[38;5;28;01mif\u001b[39;00m p.requires_grad]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mmake_loaders\u001b[39m\u001b[34m(root, batch_size, num_workers, split_seed, split_cache_path, img_size)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmake_loaders\u001b[39m(\n\u001b[32m      2\u001b[39m     root: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m      3\u001b[39m     batch_size: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m      7\u001b[39m     img_size: \u001b[38;5;28mint\u001b[39m = \u001b[32m224\u001b[39m,\n\u001b[32m      8\u001b[39m ) -> Tuple[DataLoader, DataLoader, DataLoader, SplitIndices]:\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     base_all = \u001b[43mload_flowers102_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m=\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     n = \u001b[38;5;28mlen\u001b[39m(base_all)\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m os.path.isfile(split_cache_path):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 2\u001b[39m, in \u001b[36mload_flowers102_all\u001b[39m\u001b[34m(root)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_flowers102_all\u001b[39m(root: \u001b[38;5;28mstr\u001b[39m) -> Dataset:\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFlowers102Local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m=\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mFlowers102Local.__init__\u001b[39m\u001b[34m(self, root, transform)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28mself\u001b[39m.img_dir = \u001b[38;5;28mself\u001b[39m.root / \u001b[33m\"\u001b[39m\u001b[33mjpg\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mself\u001b[39m.transform = transform\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m mat = \u001b[43msio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloadmat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimagelabels.mat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m labels = mat[\u001b[33m\"\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m\"\u001b[39m].squeeze().astype(\u001b[38;5;28mint\u001b[39m)  \u001b[38;5;66;03m# 1..102\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mself\u001b[39m.labels = (labels - \u001b[32m1\u001b[39m).tolist()           \u001b[38;5;66;03m# 0..101\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\miniconda3\\Lib\\site-packages\\scipy\\io\\matlab\\_mio.py:233\u001b[39m, in \u001b[36mloadmat\u001b[39m\u001b[34m(file_name, mdict, appendmat, spmatrix, **kwargs)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[33;03mLoad MATLAB file.\u001b[39;00m\n\u001b[32m     90\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    230\u001b[39m \u001b[33;03m    3.14159265+3.14159265j])\u001b[39;00m\n\u001b[32m    231\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    232\u001b[39m variable_names = kwargs.pop(\u001b[33m'\u001b[39m\u001b[33mvariable_names\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m233\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_open_file_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mappendmat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m    \u001b[49m\u001b[43mMR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmat_reader_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmatfile_dict\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mMR\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvariable_names\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\miniconda3\\Lib\\contextlib.py:141\u001b[39m, in \u001b[36m_GeneratorContextManager.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.kwds, \u001b[38;5;28mself\u001b[39m.func\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mgenerator didn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt yield\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\miniconda3\\Lib\\site-packages\\scipy\\io\\matlab\\_mio.py:17\u001b[39m, in \u001b[36m_open_file_context\u001b[39m\u001b[34m(file_like, appendmat, mode)\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;129m@contextmanager\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_context\u001b[39m(file_like, appendmat, mode=\u001b[33m'\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     f, opened = \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_like\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mappendmat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     19\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m f\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hp\\miniconda3\\Lib\\site-packages\\scipy\\io\\matlab\\_mio.py:47\u001b[39m, in \u001b[36m_open_file\u001b[39m\u001b[34m(file_like, appendmat, mode)\u001b[39m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_like, mode), \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m     48\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mReader needs file name or open file-like object\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     49\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: Reader needs file name or open file-like object"
     ]
    }
   ],
   "source": [
    "h1, ckpt1 = run_experih1, ckpt1 = run_experiment(\n",
    "    data_root=r\"C:\\Users\\hp\\Downloads\\102flowers\",\n",
    "    split_seed=1,\n",
    "    out_dir=\"./results/vgg19_seed1\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9634ef89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001/35 | train loss 3.1209 acc 0.2985 | val loss 1.5443 acc 0.6082 | test loss 1.4719 acc 0.6323\n",
      "Epoch 002/35 | train loss 1.3546 acc 0.6290 | val loss 0.9866 acc 0.7479 | test loss 0.9165 acc 0.7559\n",
      "Epoch 003/35 | train loss 0.8602 acc 0.7653 | val loss 0.8094 acc 0.7816 | test loss 0.7834 acc 0.7856\n",
      "Epoch 004/35 | train loss 0.6295 acc 0.8227 | val loss 0.7129 acc 0.8046 | test loss 0.6548 acc 0.8174\n",
      "Epoch 005/35 | train loss 0.4379 acc 0.8708 | val loss 0.6384 acc 0.8241 | test loss 0.5684 acc 0.8403\n",
      "Epoch 006/35 | train loss 0.3623 acc 0.8896 | val loss 0.6342 acc 0.8295 | test loss 0.5392 acc 0.8550\n",
      "Epoch 007/35 | train loss 0.3023 acc 0.9135 | val loss 0.6117 acc 0.8349 | test loss 0.5510 acc 0.8452\n",
      "Epoch 008/35 | train loss 0.2088 acc 0.9387 | val loss 0.5598 acc 0.8549 | test loss 0.4834 acc 0.8662\n",
      "Epoch 009/35 | train loss 0.2258 acc 0.9255 | val loss 0.6057 acc 0.8398 | test loss 0.5124 acc 0.8540\n",
      "Epoch 010/35 | train loss 0.1719 acc 0.9489 | val loss 0.5469 acc 0.8534 | test loss 0.4875 acc 0.8682\n"
     ]
    }
   ],
   "source": [
    "h2, ckpt2 = run_experiment(\n",
    "    data_root=r\"C:\\Users\\hp\\Downloads\\102flowers\",\n",
    "    split_seed=2,\n",
    "    out_dir=\"./results/vgg19_seed2\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
